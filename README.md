# ServoMimic Summary
My first Raspberry Pi Machine Learning using sockets to have a computer do the computationally expensive parts. <br>
The computer functions as the client socket and the Raspberry Pi (RPi) functions as the server socket. <br>
The computer takes a screenshot of video feed from a USB camera directly over the servo when it has fully turned out and the RPi sends the computer the angle of the servo, which is randomized.<br>
It eventually learns what images correspond to each angle and every certain episodes a testing cycle is done where the servo under the camera turns then the computer sends its predicted angle to the RPi to turn the second servo.

# How to get it working
- Have all the libraries used in the code for each device installed on their respective devices (most should be preinstalled, except for Tensorflow or PIL)
- Have a servo motor fastened or stuck down somewhere to where when the RPi sends a signal for it to move it doesn't move the rest of the part and where a USB camera can have constant feed of it. Also make sure that there is a fairly apparent color contrast between what the servo motor is turning (some attachment you can just put on the top gear) and the background of the image. 
- Wire the RPi correctly (will be shown in a diagram soon once the PCB website I'm using is no longer down for maintenance). All you need is 5V, ground, and an unique GPIO pin for each servo. No other components are required (if you want you could use an LCD to display loss, or something like that)
- Run the code on the RPi first, wait until the RPi prints out "Socket is listening"
- Start running the code on the computer, and then it should start learning

If you wish to terminate the code, stop it running on the RPi side, and eventually the code on the computer will quit automatically due to not recieving any tangible data. It will save the model you have if you wish to load it in for more testing cycles. 

# How it works
The first thing that the codes do is connect the sockets to one another, establishing a line of communication betwene the computer and the RPi, binding to the same port on the server (the RPi) socket side and connecting via IP addresses on the local network (the IP connection for you may be different depending on the network you use)<br>
The computer intializes the DCNN after this, with the input being the image and the output being an angle (which is normalized to be between 0 and 1, although the activation function is linear to prevent the model from quickly converging to 0. To get angle that it predicted multipy by 18, round, then multiply by 10), defining some parameters before training starts.<br> 
Then, the Raspberry Pi initializes the servo motors and the training loop starts. Every 'episode' starts with the RPi picking a random integer from 0 to 18, then multiplying that number by 10 to get the angle it turns the servo under the camera (Servo 1) to. It turns the servo motor to that angle, then sends that angle to the computer, which recieves the data and takes a screenshot of the fully-turned servo motor (the sync is very much because of the time.sleep() instances I use throughout the code to make sure every process has enough time to complete itself and gives other processes enough time to complete themselves). Both the angle that the computer gets and the image the computer screenshots are stored into repective lists for later fitting.<br>
Every 2 times batch size episodes, the computer fits the DCNN with the data it collected, sending a signal to the RPi when it done fitting. The RPi pauses its code until it recieves this signal, then both codes continue.<br>
Every 10 times batch size episodes, after the computer fits, a validation cycle occurs. In this cycle, the RPi does the same procedure it does for the start of every episode. However, the computer just takes the screenshot and does not recieve any other data. The computer then has its model predict the angle at which the servo is at, and after some multiplying and rounding to get an accurate representation of what angle the model was representing (multiplying by 18, rounding, then multiplying by 10), the computer sends the angle it predicted back to the RPi. The RPi then turns the servo not under the camera (Servo 2) to this angle. This is to see how similar the servos are, and therefore evaluate how well the model is learning. 
